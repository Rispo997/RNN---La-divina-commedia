{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "RhymeGeneration.ipynb",
      "provenance": [],
      "collapsed_sections": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "6IrBnYTaB9rV",
        "outputId": "cc847f32-fdc5-4480-a5f8-25ccf72aeb06"
      },
      "source": [
        "!pip install keras-word-char-embd\n",
        "!pip install fasttext\n",
        "from google.colab import drive\n",
        "drive.mount('/content/gdrive')"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Collecting keras-word-char-embd\n",
            "  Downloading https://files.pythonhosted.org/packages/15/5d/f6a418b22a27133251fbb260c2375c1893ae70698a097d77c4701acd19a4/keras-word-char-embd-0.21.0.tar.gz\n",
            "Requirement already satisfied: numpy in /usr/local/lib/python3.6/dist-packages (from keras-word-char-embd) (1.19.5)\n",
            "Requirement already satisfied: Keras in /usr/local/lib/python3.6/dist-packages (from keras-word-char-embd) (2.4.3)\n",
            "Requirement already satisfied: scipy>=0.14 in /usr/local/lib/python3.6/dist-packages (from Keras->keras-word-char-embd) (1.4.1)\n",
            "Requirement already satisfied: pyyaml in /usr/local/lib/python3.6/dist-packages (from Keras->keras-word-char-embd) (3.13)\n",
            "Requirement already satisfied: h5py in /usr/local/lib/python3.6/dist-packages (from Keras->keras-word-char-embd) (2.10.0)\n",
            "Requirement already satisfied: six in /usr/local/lib/python3.6/dist-packages (from h5py->Keras->keras-word-char-embd) (1.15.0)\n",
            "Building wheels for collected packages: keras-word-char-embd\n",
            "  Building wheel for keras-word-char-embd (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for keras-word-char-embd: filename=keras_word_char_embd-0.21.0-cp36-none-any.whl size=8522 sha256=59f3edf2261b7e74ab9b9d44830bfc706dd4086ce1e0a88404770cfc7448c2fb\n",
            "  Stored in directory: /root/.cache/pip/wheels/56/e1/10/a68e424326745b9ac1c0695c3da541fe6d051b129230e7456a\n",
            "Successfully built keras-word-char-embd\n",
            "Installing collected packages: keras-word-char-embd\n",
            "Successfully installed keras-word-char-embd-0.21.0\n",
            "Collecting fasttext\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/f8/85/e2b368ab6d3528827b147fdb814f8189acc981a4bc2f99ab894650e05c40/fasttext-0.9.2.tar.gz (68kB)\n",
            "\u001b[K     |████████████████████████████████| 71kB 3.4MB/s \n",
            "\u001b[?25hRequirement already satisfied: pybind11>=2.2 in /usr/local/lib/python3.6/dist-packages (from fasttext) (2.6.2)\n",
            "Requirement already satisfied: setuptools>=0.7.0 in /usr/local/lib/python3.6/dist-packages (from fasttext) (53.0.0)\n",
            "Requirement already satisfied: numpy in /usr/local/lib/python3.6/dist-packages (from fasttext) (1.19.5)\n",
            "Building wheels for collected packages: fasttext\n",
            "  Building wheel for fasttext (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for fasttext: filename=fasttext-0.9.2-cp36-cp36m-linux_x86_64.whl size=3095549 sha256=d07384473d6e64b66ed3af10d698f15debe72c31c002325db49653c5e7a04963\n",
            "  Stored in directory: /root/.cache/pip/wheels/98/ba/7f/b154944a1cf5a8cee91c154b75231136cc3a3321ab0e30f592\n",
            "Successfully built fasttext\n",
            "Installing collected packages: fasttext\n",
            "Successfully installed fasttext-0.9.2\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "x8myeYQQjNBA"
      },
      "source": [
        "from keras_wc_embd import get_embedding_layer, get_dicts_generator, get_batch_input\n",
        "from keras.preprocessing.text import Tokenizer\n",
        "from keras import regularizers\n",
        "from keras.preprocessing.sequence import pad_sequences\n",
        "from keras.utils import to_categorical\n",
        "from keras.models import Sequential, Model\n",
        "from keras.layers import Embedding, Dense, LSTM, Dropout, concatenate, Input\n",
        "from keras.callbacks import ModelCheckpoint\n",
        "from keras.callbacks import EarlyStopping\n",
        "from keras.optimizers import Adam,RMSprop\n",
        "from keras.metrics import CategoricalCrossentropy\n",
        "import fasttext as ft\n",
        "from pickle import dump\n",
        "import numpy as np\n",
        "import random \n",
        "from keras.callbacks import Callback\n",
        "from matplotlib import pyplot as plt\n",
        "from IPython.display import clear_output\n",
        "\n",
        "class PlotLearning(Callback):\n",
        "    def on_train_begin(self, logs={}):\n",
        "        self.i = 0\n",
        "        self.x = []\n",
        "        self.losses = []\n",
        "        self.val_losses = []\n",
        "        self.acc = []\n",
        "        self.val_acc = []\n",
        "        self.fig = plt.figure()\n",
        "        \n",
        "        self.logs = []\n",
        "\n",
        "    def on_epoch_end(self, epoch, logs={}):\n",
        "        \n",
        "        self.logs.append(logs)\n",
        "        self.x.append(self.i)\n",
        "        self.losses.append(logs.get('loss'))\n",
        "        self.val_losses.append(logs.get('val_loss'))\n",
        "        self.acc.append(logs.get('categorical_crossentropy'))\n",
        "        self.val_acc.append(logs.get('val_categorical_crossentropy'))\n",
        "        self.i += 1\n",
        "        f, (ax1, ax2) = plt.subplots(1, 2, sharex=True)\n",
        "        \n",
        "        clear_output(wait=True)\n",
        "        \n",
        "        ax1.set_yscale('log')\n",
        "        ax1.plot(self.x, self.losses, label=\"loss\")\n",
        "        ax1.plot(self.x, self.val_losses, label=\"val_loss\")\n",
        "        ax1.legend()\n",
        "        \n",
        "        ax2.plot(self.x, self.acc, label=\"cross entropy\")\n",
        "        ax2.plot(self.x, self.val_acc, label=\"validation cross entropy\")\n",
        "        ax2.legend()\n",
        "\n",
        "        plt.show();"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "4ju2e8_3cgvk"
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "n1ZAWo22rrX8"
      },
      "source": [
        "def check_rhyme(first,second):\n",
        "  first_word = normalizeWord(first)\n",
        "  second_word = normalizeWord(second)\n",
        "  vowels = ['a','e','i','o','u', 'à', 'è', 'é', 'ì', 'ò', 'ó', 'ù']\n",
        "  #check if the two words are equal from the end to the second occured vowel\n",
        "  for i in range(2, min(len(first_word), len(second_word))-1):\n",
        "    if first_word[-i] != second_word[-i]:  \n",
        "      return False\n",
        "    if i != 1 and (first_word[-i] in vowels or second_word[-i] in vowels):\n",
        "      break\n",
        "  return True\n",
        "\n",
        "def score_rhyme(first, second, enlight_end=1): \n",
        "    weight = enlight_end\n",
        "    first_word = normalizeWord(first)\n",
        "    second_word = normalizeWord(second)\n",
        "    vowels = ['a','e','i','o','u', 'à', 'è', 'é', 'ì', 'ò', 'ó', 'ù']\n",
        "    #check if the two words are equal from the end to the second occured vowel\n",
        "    score = not (first_word[-1] == second_word[-1])/weight if (len(first_word) and len(second_word)) else 0\n",
        "    maximum = 1\n",
        "    for i in range(2, min(len(first_word), len(second_word))-1):\n",
        "      if (first_word[-i] != second_word[-i]): \n",
        "        score += weight\n",
        "        weight /= enlight_end\n",
        "      if (first_word[-i] in vowels or second_word[-i] in vowels):\n",
        "        break\n",
        "    return score\n",
        "\n",
        "#get the last verse's word normalized \n",
        "def normalizeWord(word):\n",
        "    return word.strip('’)').lower()\n",
        "\n",
        "\n",
        "#get a new random value from sequence\n",
        "def getNewRandom(old,sequence):\n",
        "  new = random.choice(sequence)\n",
        "  while new == old:\n",
        "    new = random.choice(sequence)\n",
        "  return new\n",
        "\n",
        "def getSyllable(word):\n",
        "  vowels = ['a','e','i','o','u', 'à', 'è', 'é', 'ì', 'ò', 'ó', 'ù']\n",
        "  for i in range(2, len(word)):\n",
        "    if word[-i] in vowels:\n",
        "      return word[-i:]\n",
        "    "
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "dkh3zaMitt6G"
      },
      "source": [
        "  def generateRhymes(vocab,text,length = 30000):\n",
        "    \n",
        "    raw = [verse for verse in text if verse != \"<tercet>\"]\n",
        "    uniquewords = [word for word in vocab.word_index.keys() if len(word) > 2 and word != \"_unk_\"]\n",
        "    # print(words)\n",
        "    rhymes_dict = dict()\n",
        "    for word in uniquewords:\n",
        "      syllable = getSyllable(word)\n",
        "      rhymes_dict.setdefault(syllable,[]).append(word)\n",
        "\n",
        "    # Remove rhyming patterns with only one word\n",
        "    for rhyme in list(rhymes_dict):\n",
        "      if len(rhymes_dict[rhyme]) < 2:\n",
        "        rhymes_dict.pop(rhyme)\n",
        "\n",
        "    print(\"The number of rhyming patterns is: \",len(rhymes_dict))\n",
        "    rhymes = []\n",
        "    token = \"<tercet>\"\n",
        "    curr_syll = random.choice(list(rhymes_dict.keys()))\n",
        "    print(curr_syll)\n",
        "    for i in range(length):\n",
        "      # Get next rhyming pattern\n",
        "      next_syll = getNewRandom(curr_syll,list(rhymes_dict.keys()))\n",
        "      # Fetch words according to pattern\n",
        "      first = random.choice(rhymes_dict[curr_syll])\n",
        "      second = random.choice(rhymes_dict[next_syll])\n",
        "      third = getNewRandom(first,rhymes_dict[curr_syll])\n",
        "      # Update rhyming pattern\n",
        "      curr_syll = next_syll\n",
        "      # Update output sequence\n",
        "      rhymes.append(first)\n",
        "      rhymes.append(second)\n",
        "      rhymes.append(third)\n",
        "      rhymes.append(token)\n",
        "    return rhymes\n",
        "\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "CgV3UEg4jXml"
      },
      "source": [
        "#uploaded = files.upload()\n",
        "\n",
        "# Open the file and encode newlines as standalone symbols\n",
        "with open(\"DC-poem-formatUTF_withoutBOL.txt\", encoding='latin-1') as file:\n",
        "  text = file.read().lower()\n",
        "text = text.split('\\n')\n",
        "text = [string for string in text]\n",
        "print(text)\n",
        "print('Number of Characters is:', len(text))\n",
        "\n",
        "#Initialize parameters\n",
        "SEQUENCE_LEN = 3 # The length of each sequence used to predict\n",
        "STEP = 1 # Stride\n",
        "X = [] # Input Variables \n",
        "Y = [] # Output Variables\n",
        "SEPARATOR_CHAR_TOKEN = '$'\n",
        "\n",
        "# Fetch all the words inside the file\n",
        "words = []\n",
        "for verse in text:\n",
        "  if verse[-1] == \" \":\n",
        "    verse = verse[:-1]   \n",
        "  verse_split = verse.split(\" \")\n",
        "  words.append(verse_split[-1])\n",
        "print(words)\n",
        "words = words[1:]\n",
        "# Fit the Tokenizer\n",
        "tokenizer_word = Tokenizer(filters=[], lower=True, oov_token=\"_unk_\")\n",
        "tokenizer_word.fit_on_texts(text)\n",
        "word_dict_from_idx = {v:k for k,v in tokenizer_word.word_index.items()}\n",
        "word_dict_from_idx[0] = '' "
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "kYJCEO9x87FY"
      },
      "source": [
        "dummyRhymes = generateRhymes(tokenizer_word,text,10000)\n",
        "words.extend(dummyRhymes)\n",
        "print(words[:100])\n",
        "print(words[-100:])\n",
        "words_tokenized = tokenizer_word.texts_to_sequences(words)\n",
        "vocab_size = len(tokenizer_word.word_index) + 1\n",
        "dump(tokenizer_word, open('generaRimaWord.pkl', 'wb'))\n",
        "print('Number of Words is:', vocab_size)\n",
        "\n",
        "tokenizer_char = Tokenizer(filters=[], lower=True, oov_token=\"_unk_\", char_level=True)\n",
        "tokenizer_char.fit_on_texts(words + [SEPARATOR_CHAR_TOKEN])\n",
        "character_num = len(tokenizer_char.word_index) + 1\n",
        "print('Number of Characters is:', character_num)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "aoY0GMRm8ZBb"
      },
      "source": [
        "# Flatten the resulting List\n",
        "words_tokenized = [item for sublist in words_tokenized for item in sublist]\n",
        "\n",
        "#padding_token_val = words_tokenized[0]\n",
        "start_token_val = words_tokenized[0]\n",
        "\n",
        "X = []\n",
        "max_word_len = 0\n",
        "print(len(words))\n",
        "print(len(words_tokenized))\n",
        "for i in range(3, len(words_tokenized), STEP):\n",
        "  max_word_len = max([max_word_len, len(words[i-3]), len(words[i-2]), len(words[i-1])])\n",
        "  X.append([words_tokenized[i-3],words_tokenized[i-2],words_tokenized[i-1]])\n",
        "print('Number of Sequences:', len(X), len(Y))\n",
        "\n",
        "Y = []\n",
        "for i in range(3, len(words_tokenized), STEP):\n",
        "  Y.append(words_tokenized[i])\n",
        "\n",
        "X = np.array(X)\n",
        "Y = np.array(Y)\n",
        "print(X[:10])\n",
        "Y = to_categorical(Y, num_classes=vocab_size)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "QdYko1FBHgYZ"
      },
      "source": [
        "from keras.losses import categorical_crossentropy\n",
        "from tensorflow.math import add, reduce_sum\n",
        "from tensorflow import constant, float32\n",
        "import tensorflow\n",
        "#Loss\n",
        "from keras.backend import argmax, get_value\n",
        "from tensorflow import make_ndarray, make_tensor_proto \n",
        "\n",
        "def convert_tensor_to_list_of_words(tensor):\n",
        "  return [word_dict_from_idx[idx] for idx in make_ndarray(make_tensor_proto(argmax(tensor)))]\n",
        "\n",
        "def custom_loss(y_true, y_pred):\n",
        "  rhyme_weight = 10\n",
        "  true = convert_tensor_to_list_of_words(y_true)\n",
        "  pred = convert_tensor_to_list_of_words(y_pred)\n",
        "  rhymes_scores = [rhyme_weight*score_rhyme(t, p, enlight_end=1) for t,p in zip(true, pred)]\n",
        "  s = constant(rhymes_scores, dtype=float32)\n",
        "  cat_loss = categorical_crossentropy(y_true, y_pred, from_logits=False)\n",
        "  # tensorflow.print(cat_loss.size)\n",
        "  return add(s, cat_loss) "
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "eCY06rVULu-y"
      },
      "source": [
        "# import tensorflow as tf\r\n",
        "\r\n",
        "# tpu = tf.distribute.cluster_resolver.TPUClusterResolver()\r\n",
        "# tf.config.experimental_connect_to_cluster(tpu)\r\n",
        "# tf.tpu.experimental.initialize_tpu_system(tpu)\r\n",
        "# strategy = tf.distribute.experimental.TPUStrategy(tpu)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "99cf18cDHqX9"
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "vQj1ET6vRpoD"
      },
      "source": [
        "ftmodel = ft.train_unsupervised(input='DC-poem-formatUTF_withoutBOL.txt', minn=3, maxn=6, dim=100, epoch=20)\r\n",
        "embedding_dim = 100\r\n",
        "def build_embedding_matrix(vocabulary,embedding_dimension):\r\n",
        "  numWords = len(vocabulary.word_index) + 1\r\n",
        "  embedding = np.empty(shape=(numWords,embedding_dimension))\r\n",
        "  for word in vocabulary.word_index:\r\n",
        "      embedding[vocabulary.word_index[word]] = ftmodel.get_word_vector(word)\r\n",
        "  return embedding\r\n",
        "embedding_matrix = build_embedding_matrix(tokenizer_word,embedding_dim)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "7g90xu75WAB3"
      },
      "source": [
        "print(X.shape)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "K55Ah2c7nZnt"
      },
      "source": [
        "# Create the neural network\n",
        "model_filepath=\"/content/gdrive/My Drive/generaword/weights-{epoch:02d}-{loss:.4f}.hdf5\"\n",
        "input_words = Input(shape=X.shape[1])\n",
        "embedding_words = Embedding(vocab_size, embedding_dim,input_length=X.shape[1], mask_zero=True,weights = [embedding_matrix],trainable=False)(input_words)\n",
        "layer1 = LSTM(128, dropout=0.2)(embedding_words)\n",
        "dense_softmax = Dense(vocab_size, activation='softmax')(layer1)\n",
        "model = Model(input_words, dense_softmax)\n",
        "print(model.summary())"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Klg3rsc8AAOg"
      },
      "source": [
        "from tensorflow.keras.models import load_model\n",
        "\n",
        "model.compile(loss=\"categorical_crossentropy\", optimizer='adam', metrics=['accuracy'], run_eagerly=True)\n",
        "# Define Callbacks\n",
        "filename = \"/content/model-GeneraRimaWord60Epoch.h5\"\n",
        "model = load_model(filename,custom_objects={'custom_loss': custom_loss})\n",
        "model.load_weights(filename)\n",
        "es = EarlyStopping(monitor=custom_loss, mode='min', verbose=2, patience=150)\n",
        "ck = ModelCheckpoint(model_filepath, monitor=custom_loss, verbose=1, mode='min')\n",
        "# Start Training\n",
        "model.fit(X, Y,shuffle=False, epochs=200, callbacks=[es,ck,PlotLearning()])\n",
        "model.save('/content/gdrive/My Drive/Dante_model/model.h5')\n",
        "\n",
        "model.save('model-last.h5')\n",
        "\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "RNKNoj2rRo0_"
      },
      "source": [
        "model.save('model-GeneraRimaWord180Epoch.h5')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Aq2t1fe8cMgp"
      },
      "source": [
        "model.compile(loss=custom_loss, optimizer='adam', metrics=['categorical_crossentropy'], run_eagerly=True)\n",
        "print(model.summary())\n",
        "# Define Callbacks\n",
        "es = EarlyStopping(monitor=custom_loss, mode='min', verbose=2, patience=150)\n",
        "\n",
        "ck = ModelCheckpoint(model_filepath, monitor=custom_loss, verbose=1, mode='min')\n",
        "model.fit([X_word, X_char], Y, batch_size=128, epochs=50, callbacks=[ck,PlotLearning()], validation_split = 0)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "O3HLTr5qthTW"
      },
      "source": [
        "model_loaded = model"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "0bjWTZqXusY6"
      },
      "source": [
        "from pickle import load\n",
        "from keras.models import load_model\n",
        "import numpy as np \n",
        "def sample(preds, temperature=1.0):\n",
        "  # helper function to sample an index from a probability array\n",
        "  preds = np.asarray(preds).astype('float64')\n",
        "  preds = np.log(preds) / temperature\n",
        "  exp_preds = np.exp(preds)\n",
        "  preds = exp_preds / np.sum(exp_preds)\n",
        "  probas = np.random.multinomial(1, preds, 1)\n",
        "  return np.argmax(probas)\n",
        "\n",
        "# # Initialize variables\n",
        "n_words = 100\n",
        "predicted_word = ''\n",
        "generate = []\n",
        "\n",
        "# Load pre-trained data\n",
        "tokenizer_word = load(open('/content/generaRimaWord.pkl', 'rb'))\n",
        "#model_loaded = load_model('/content/100epocheGeneraRimaWord.h5', custom_objects={'custom_loss': custom_loss})\n",
        "# Encode initial input\n",
        "previous = ['<start>', 'vita', 'oscura']\n",
        "previous = [tokenizer_word.word_index[word] for word in previous]\n",
        "print(previous)\n",
        "#while predicted_word != '_end_':\n",
        "for i in range(n_words):\n",
        "  prediction = model.predict([previous], verbose=0)\n",
        "  predicted_word = tokenizer_word.sequences_to_texts([[np.argmax(prediction)]])\n",
        "  previous.append(tokenizer_word.word_index[predicted_word[0]])\n",
        "  previous = previous[-3:]\n",
        "  generate.append(predicted_word)\n",
        "  print(predicted_word)\n",
        "# Format and print the result\n",
        "generated = ['\\n' if x=='_verse_' or x=='_end_' else x for x in generated]\n",
        "output = ' '.join(generated)\n",
        "print(output)\n",
        "#with open(\"output/seq50epoch100.txt\", \"w\") as file:\n",
        "#\tfile.write(output)"
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}