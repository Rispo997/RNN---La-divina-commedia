# -*- coding: utf-8 -*-
"""Dante.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/1JEOAjAZLP7bJt1uXLtFX0NIPozZGlIOs
"""

# from google.colab import drive
# drive.mount('/content/gdrive')

from keras.preprocessing.text import Tokenizer
from keras import regularizers
from keras.preprocessing.sequence import pad_sequences
from keras.utils import to_categorical
from keras.models import Sequential
from keras.layers import Dense
from keras.layers import LSTM
from keras.layers import Embedding
from keras.layers import Dropout
from keras.callbacks import ModelCheckpoint
from keras.callbacks import EarlyStopping
from keras.optimizers import Adam
from keras.metrics import CategoricalCrossentropy
from pickle import dump
import numpy as np
# from google.colab import files
# To Do
# - Testare differenti tipi di architetture per la rete neurale (Layers,Activation function,dropout,batch size,epochs)
# - Testare differenti tipi di SEQUENCE_LEN
# - Valutare l'idea di eliminare le parole meno utilizzate
# - Valutare l'idea di un'approccio character-based

from keras.callbacks import Callback

from matplotlib import pyplot as plt
from IPython.display import clear_output

class PlotLearning(Callback):
    def on_train_begin(self, logs={}):
        self.i = 0
        self.x = []
        self.losses = []
        self.val_losses = []
        self.acc = []
        self.val_acc = []
        self.fig = plt.figure()
        
        self.logs = []

    def on_epoch_end(self, epoch, logs={}):
        
        self.logs.append(logs)
        self.x.append(self.i)
        self.losses.append(logs.get('loss'))
        self.val_losses.append(logs.get('val_loss'))
        self.acc.append(logs.get('categorical_crossentropy'))
        self.val_acc.append(logs.get('val_categorical_crossentropy'))
        self.i += 1
        f, (ax1, ax2) = plt.subplots(1, 2, sharex=True)
        
        clear_output(wait=True)
        
        ax1.set_yscale('log')
        ax1.plot(self.x, self.losses, label="loss")
        ax1.plot(self.x, self.val_losses, label="val_loss")
        ax1.legend()
        
        ax2.plot(self.x, self.acc, label="cross entropy")
        ax2.plot(self.x, self.val_acc, label="validation cross entropy")
        ax2.legend()
        
        plt.show();

# uploaded = files.upload()

# Open the file and encode newlines as standalone symbols
with open("resources/DC-poem-format.txt", encoding='latin-1') as file:
    text = file.read().lower()

print('Number of Characters is:', len(text))

#Initialize parameters
SEQUENCE_LEN = 50 # The length of each sequence used to predict
STEP = 1 # Stride
X = [] # Input Variables 
Y = [] # Output Variables

# Fetch all the words inside the file
words = []
#padding_token = "_pad_"
start_token = "_start_"
#for w in text.split(' '):
  #if w.strip() != '' or w != '\n':
    #if(w == start_token):
      #for i in range(SEQUENCE_LEN-1):
        #words.append(padding_token) 
    #words.append(w)

words = [w for w in text.split(' ') if w.strip() != '' or w != '\n']
print('The length of the text:', len(words))

# Encode the words using integers
tokenizer = Tokenizer(filters=[], lower=True, oov_token="_unk_")
tokenizer.fit_on_texts(words)
words_tokenized = tokenizer.texts_to_sequences(words)
vocab_size = len(tokenizer.word_index) + 1
print('Number of Words is:', vocab_size)

# Flatten the resulting List
words_tokenized = [item for sublist in words_tokenized for item in sublist]

#padding_token_val = words_tokenized[0]
start_token_val = words_tokenized[0]

for i in range(1, len(words_tokenized), STEP):
  #do not predict start token
  tmp_sequence = []
  if words_tokenized[i] != start_token_val:
    # get the SEQUENCE_LEN token before the one to predict, avoiding out of bound access and starting from the start token
    lower_bound = i-(SEQUENCE_LEN) if i-(SEQUENCE_LEN) >= 0 else 0
    tmp_sequence = words_tokenized[lower_bound:i]
    try:
      index = tmp_sequence.index(start_token_val)
      tmp_sequence = tmp_sequence[index:]
    except:
      pass 
    tmp_sequence = pad_sequences([tmp_sequence], maxlen=SEQUENCE_LEN, truncating='pre', padding='pre',value=0)[0]
    tmp_sequence = [x for x in tmp_sequence]
    X.append(tmp_sequence)
    Y.append(words_tokenized[i])
print('Number of Sequences:', len(X), len(Y))

X = np.array(X)
Y = np.array(Y)
Y = to_categorical(Y, num_classes=vocab_size)

# Create the neural network
model_filepath="weights-{epoch:02d}-{loss:.4f}.hdf5"
model = Sequential()
model.add(Embedding(vocab_size, 50, input_length=X.shape[1], mask_zero=True))
model.add(LSTM(100, return_sequences=True))
model.add(Dropout(0.2))
model.add(LSTM(100))
model.add(Dropout(0.2))
model.add(Dense(100, activation='relu'))
model.add(Dense(vocab_size, activation='softmax'))
model.compile(loss='categorical_crossentropy', optimizer=Adam(), metrics=['categorical_crossentropy'])
print(model.summary())

# Define Callbacks
es = EarlyStopping(monitor='categorical_crossentropy', mode='min', verbose=2, patience=5)
ck = ModelCheckpoint(model_filepath, monitor='categorical_crossentropy', verbose=1, save_best_only=True, mode='min')

# Start Training
model.fit(X, Y, batch_size=128, epochs=100, callbacks=[es,ck,PlotLearning()])
#model.save('/content/gdrive/My Drive/Dante_model/model.h5')
model.save('model-last.h5')