# -*- coding: utf-8 -*-
"""ClassifyRhyme

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/19FWVVTapRW2PUD_Tc55XyHyPjt53_2GT

### File loading and import
"""

from google.colab import drive
drive.mount('/content/gdrive')

from google.colab import files
import json
with open("/content/gdrive/My Drive/DL_project/rhyme_classification.json", encoding='latin-1') as file:
    rhymes_dict = json.load(file)

import numpy as np
from tensorflow.keras.layers import Embedding, Dense, Input, Dropout, TimeDistributed, Flatten, LSTM, Bidirectional, GlobalAveragePooling1D, Lambda
from tensorflow.keras.models import Model, Sequential
from tensorflow.keras.utils import to_categorical
from tensorflow.keras.preprocessing.text import Tokenizer
from tensorflow.keras.preprocessing.sequence import pad_sequences
from tensorflow.keras.callbacks import EarlyStopping
from sklearn.utils import shuffle
from sklearn.model_selection import train_test_split
from sklearn.metrics import accuracy_score, f1_score
from sklearn.metrics.pairwise import cosine_similarity

"""### Prepare dataset"""

words = []
Y = []
rhyme_idx = dict()
for i,rhyme in enumerate(rhymes_dict):
  rhyme_idx[i] = rhyme
  for word in rhymes_dict[rhyme]:
    words.append(word)
    Y.append(i)

num_rhymes = len(rhymes_dict)
vocab_size = len(words)

tokenizer = Tokenizer(num_words=vocab_size, lower=True, split=' ', char_level=True, oov_token=None)
tokenizer.fit_on_texts(words)
X = pad_sequences(tokenizer.texts_to_sequences(words), padding='pre', value=0)

X = np.array(X)
Y = to_categorical(Y, num_classes=num_rhymes)
print(X.shape)
print(Y.shape)

X, Y = shuffle(X, Y)

X_train, X_test, Y_train, Y_test = train_test_split(X, Y, test_size=0.2)

"""### Plot learning graph"""

from keras.callbacks import Callback

from matplotlib import pyplot as plt
from IPython.display import clear_output

class PlotLearning(Callback):
    def on_train_begin(self, logs={}):
        self.i = 0
        self.x = []
        self.losses = []
        self.val_losses = []
        self.acc = []
        self.val_acc = []
        self.fig = plt.figure()
        
        self.logs = []

    def on_epoch_end(self, epoch, logs={}):
        
        self.logs.append(logs)
        self.x.append(self.i)
        self.losses.append(logs.get('loss'))
        self.val_losses.append(logs.get('val_loss'))
        self.acc.append(logs.get('categorical_accuracy'))
        self.val_acc.append(logs.get('val_categorical_accuracy'))
        self.i += 1
        f, (ax1, ax2) = plt.subplots(1, 2, sharex=True)
        
        clear_output(wait=True)
        
        ax1.set_yscale('log')
        ax1.plot(self.x, self.losses, label="loss")
        ax1.plot(self.x, self.val_losses, label="val_loss")
        ax1.legend()
        
        ax2.plot(self.x, self.acc, label="categorical accuracy")
        ax2.plot(self.x, self.val_acc, label="val categorical accuracy")
        ax2.legend()

        plt.show();

"""### Models

#### Embeddings model
"""

embeddding_size = 30

input = Input(shape=(X_train.shape[1]))
embedding = Embedding(vocab_size, embeddding_size, input_length=X_train.shape[1], mask_zero=True, embeddings_initializer='zeros')(input)
flatten = Flatten()(embedding)
#encoding = Dense(10, kernel_initializer='zeros', kernel_regularizer='l2')(flatten)
#encoding = LSTM(10)(embedding)
dense = Dense(num_rhymes, activation='softmax', kernel_regularizer='l2', trainable=True)(flatten)
dropout = Dropout(0.2)(dense)
model = Model(input, dropout)
print(model.summary())

model.compile(loss='categorical_crossentropy', optimizer='adam', metrics=['categorical_accuracy'])
es = EarlyStopping(monitor='categorical_accuracy', mode='max', verbose=2, patience=10)
# Start Training
model.fit(X_train, Y_train, batch_size=128, epochs=50, callbacks=[PlotLearning()], validation_split = 0.25, shuffle=True)

predicted = np.array([np.argmax(p) for p in model.predict(X_test)])
true = np.array([np.argmax(t) for t in Y_test])
accuracy = accuracy_score(true, predicted)
f1score = f1_score(true, predicted, average='macro')
print(accuracy)
print(f1score)

embedding_model = Model(input, embedding, name="Embedding")
embedding_model.summary()
embedding_model.save("EmbeddingModelRhyme.h5")

from tensorflow.keras.models import load_model
embedding_model = load_model("EmbeddingModelRhyme.h5")

"""#### Encoding model"""

encdoding_dimension = 10
input = Input(shape=(X_train.shape[1]), name="Input word")
embedding = embedding_model(input)
flatten = Flatten()(embedding)
#gpa = GlobalAveragePooling1D()(embedding)
encoding = Dense(encdoding_dimension, kernel_initializer='zeros', name="Encoder")(flatten)
dense = Dense(num_rhymes, activation='softmax', kernel_regularizer='l2', trainable=True, name="Rhyme_classifier")(encoding)
dropout = Dropout(0.2)(dense)
model = Model(input, dense)
print(model.summary())

model.compile(loss='categorical_crossentropy', optimizer='adam', metrics=['categorical_accuracy'])
es = EarlyStopping(monitor='categorical_accuracy', mode='max', verbose=2, patience=10)
# Start Training
model.fit(X_train, Y_train, batch_size=128, epochs=10, callbacks=[PlotLearning()], validation_split = 0.25, shuffle=True)

predicted = np.array([np.argmax(p) for p in model.predict(X_test)])
true = np.array([np.argmax(t) for t in Y_test])
accuracy = accuracy_score(true, predicted)
f1score = f1_score(true, predicted, average='macro')
print(accuracy)
print(f1score)

encoding_model = Model(input, encoding)
encoding_model.summary()
encoding_model.save("EncodingModelRhyme100eEmbedding300e.h5")

"""### Metrics"""

rhyme_scores = {rhyme: cosine_similarity([getEmbeddingVector(word) for word in rhymes_dict[rhyme]]) for i,rhyme in enumerate(rhymes_dict)}

mean_rhyme_scores = dict()
for rhyme in rhyme_scores:
  mean_rhyme_scores[rhyme] = np.mean(rhyme_scores[rhyme])
scores = np.array([s for s in mean_rhyme_scores.values()])
len(scores[scores>0.5])/len(scores)

import random

def getEmbeddingVector(word):
  return encoding_model.predict(pad_sequences([tokenizer.texts_to_sequences(word)], maxlen=16, padding='pre', value=0))[0]

def getSimilarity(word1, word2):
  w1 = getEmbeddingVector(word1).reshape(1, 10)
  w2 = getEmbeddingVector(word2).reshape(1, 10)
  return cosine_similarity(w1, w2)[0,0]

def printInfo(scores, rhyme=True):
  TAG = "" if rhyme else "non "
  print("Numero parole testate: ", scores.shape[0])
  print("Media similarità parole "+TAG+"in rima: ", np.mean(scores))
  print("Percentuale parole similarità sopra 0.3: ", len(scores[scores>0.3])/len(scores))

def compare(e1, e2, equal):
  return e1 == e2 if equal else e1 != e2

def getScores(words, words_to_rhymes, rhymes=True, num_words=100):
  scores = []
  for i in range(num_words):
    random1 = random.randint(0,len(words)-1)
    random2 = random.randint(0,len(words)-1)
    while words[random1] not in words_to_rhymes or words[random2] not in words_to_rhymes or (compare(words_to_rhymes[words[random1]],  words_to_rhymes[words[random2]], not rhymes)):
      random1 = random.randint(0,len(words)-1)
      random2 = random.randint(0,len(words)-1)
    w1 = words[random1]
    w2 = words[random2]
    s = getSimilarity(w1, w2)
    scores.append(s)
  return np.array(scores)

words_to_rhymes = {w:k for k,l in rhymes_dict.items() for w in l}

scores_rhyme = getScores(words, words_to_rhymes, True)
scores_not_rhyme = getScores(words, words_to_rhymes, False, 1000)

printInfo(scores_rhyme)
printInfo(scores_not_rhyme, False)