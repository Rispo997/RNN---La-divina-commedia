{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "name": "DeepComedy_Word-Char_Seq2Seq.ipynb",
      "provenance": [],
      "collapsed_sections": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "tR_32XmXKb1T"
      },
      "source": [
        "All info here: https://blog.keras.io/a-ten-minute-introduction-to-sequence-to-sequence-learning-in-keras.html\n",
        "\n",
        "Code here: https://github.com/keras-team/keras/blob/master/examples/lstm_seq2seq.py"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "te_vFzj9pCkR"
      },
      "source": [
        "## Import"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "N_TzkL_Fz3qX",
        "outputId": "8b3aee2c-e437-4fd9-bf9e-f99f33df5d0d"
      },
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/gdrive')"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Mounted at /content/gdrive\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "DeukqcgeKEB6"
      },
      "source": [
        "from google.colab import files\n",
        "from tensorflow.keras.preprocessing.text import Tokenizer\n",
        "from tensorflow.keras.utils import to_categorical\n",
        "from tensorflow.keras.models import Model, load_model\n",
        "from tensorflow.keras.layers import Dense, LSTM, Input, Embedding, Dropout, Attention, Bidirectional, Lambda, TimeDistributed, Concatenate, Dot\n",
        "from tensorflow.keras.callbacks import Callback, ModelCheckpoint, EarlyStopping\n",
        "import numpy as np\n",
        "from tensorflow.keras.preprocessing.sequence import pad_sequences\n",
        "\n",
        "from matplotlib import pyplot as plt\n",
        "from IPython.display import clear_output\n",
        "\n",
        "class PlotLearning(Callback):\n",
        "    def on_train_begin(self, logs={}):\n",
        "        self.i = 0\n",
        "        self.x = []\n",
        "        self.losses = []\n",
        "        self.val_losses = []\n",
        "        self.acc = []\n",
        "        self.val_acc = []\n",
        "        self.fig = plt.figure()\n",
        "        \n",
        "        self.logs = []\n",
        "\n",
        "    def on_epoch_end(self, epoch, logs={}):\n",
        "        \n",
        "        self.logs.append(logs)\n",
        "        self.x.append(self.i)\n",
        "        self.losses.append(logs.get('loss'))\n",
        "        self.val_losses.append(logs.get('val_loss'))\n",
        "        self.acc.append(logs.get('categorical_accuracy'))\n",
        "        self.val_acc.append(logs.get('val_categorical_accuracy'))\n",
        "        self.i += 1\n",
        "        f, (ax1, ax2) = plt.subplots(1, 2, sharex=True)\n",
        "        \n",
        "        clear_output(wait=True)\n",
        "        \n",
        "        ax1.set_yscale('log')\n",
        "        ax1.plot(self.x, self.losses, label=\"crossentropy\")\n",
        "        ax1.plot(self.x, self.val_losses, label=\"val_crossentropy\")\n",
        "        ax1.legend()\n",
        "        \n",
        "        ax2.plot(self.x, self.acc, label=\"accuracy\")\n",
        "        ax2.plot(self.x, self.val_acc, label=\"val_accuracy\")\n",
        "        ax2.legend()\n",
        "        \n",
        "        plt.show();"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "fcyeP4iOn41H"
      },
      "source": [
        "# from google.colab import files\n",
        "# uploaded = files.upload()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "KdN5_yw2pFjn"
      },
      "source": [
        "## Preparation of the dataset"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ddlpTeDTpP0e"
      },
      "source": [
        "### Compute inputs (for encoder and decoder) and target"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "2HNgu6COSBpR"
      },
      "source": [
        "def tokenizationForRhymeEncoding(word_to_idx, max_len_word):\r\n",
        "  words_tokenized = dict()\r\n",
        "  words_tokenized[0] = np.zeros(max_len_word, dtype=np.int32)\r\n",
        "  #Encoding of the rhyme (OPTIONAL)\r\n",
        "  char_to_idx = {\"'\": 17, '(': 26, ')': 24, 'a': 1,  'b': 19, 'c': 9, 'd': 11, 'e': 3, 'f': 18, 'g': 14, 'h': 20, 'i': 2, 'j': 25, 'l': 10, 'm': 12, 'n': 6, 'o': 4, 'p': 13, 'q': 22, 'r': 5, 's': 8, 't': 7, 'u': 15, 'v': 16, 'x': 23, 'y': 27, 'z': 21}\r\n",
        "  for (word,idx) in word_to_idx.items():\r\n",
        "    char_list = []\r\n",
        "    for c in word:\r\n",
        "      char_list.append(char_to_idx.get(c, 0))\r\n",
        "    words_tokenized[idx] = pad_sequences([char_list], maxlen=16, value = 0)[0]\r\n",
        "  return words_tokenized"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "mCh5gwEHokw1"
      },
      "source": [
        "#Prepare dataset for training: input for encoder and decoder and target for decoder\n",
        "CHARACTER_BASED = False\n",
        "RHYME_ENCODING = False and (not CHARACTER_BASED)\n",
        "LINE_SEPARATOR = '$'\n",
        "encoder_input_texts = []\n",
        "decoder_input_texts = []\n",
        "decoder_target_texts = []\n",
        "NUM_VERSES_PREVIOUS_CONTEXT = 3\n",
        "max_encoder = 0\n",
        "max_decoder = 0\n",
        "word_max_len = 16\n",
        "\n",
        "with open(\"/content/gdrive/My Drive/DC-poem-formatUTF_withTercet.txt\", encoding='latin-1') as file:\n",
        "  text = file.read()\n",
        "\n",
        "START = \"1\" if CHARACTER_BASED else \"<start>\"\n",
        "END = \"2\" if CHARACTER_BASED else \"<end>\"\n",
        "TERCET = \"3\" if CHARACTER_BASED else \"<tercet>\"\n",
        "BOL = \"4\" if CHARACTER_BASED else \"<bol>\" \n",
        "EOL = \"5\" if CHARACTER_BASED else \"<eol>\"\n",
        "\n",
        "if CHARACTER_BASED:\n",
        "  text = text.replace(\"<start>\", START).replace(\"<end>\", END).replace(\"<tercet>\", TERCET).replace(\"<bol>\", BOL).replace(\"<eol>\", EOL)\n",
        "else:\n",
        "  text = text.replace(BOL, BOL + \" \").replace(EOL, \" \" + EOL)\n",
        "datasetTexts = text.split('\\n')\n",
        "\n",
        "tokenizer = Tokenizer(char_level=CHARACTER_BASED, filters='!\"#%&()*+,-./:;=?@[\\\\]^_`{|}~\\t\\n', oov_token=\"<oov_token>\")\n",
        "tokenizer.fit_on_texts(datasetTexts + [LINE_SEPARATOR])\n",
        "token_to_idx = tokenizer.word_index\n",
        "vocab_size = len(tokenizer.word_index) + 1\n",
        "tokenized_datasetTexts = tokenizer.texts_to_sequences(datasetTexts)\n",
        "\n",
        "if RHYME_ENCODING:\n",
        "  rhymeEncodingInputByWord = tokenizationForRhymeEncoding(token_to_idx, word_max_len)\n",
        "  rhyme_model = load_model('/content/gdrive/My Drive/RhymeEncoding.h5', compile=False) \n",
        "\n",
        "for i in range(0, len(tokenized_datasetTexts)):\n",
        "  #encoder inputs (compute previous context of a verse)\n",
        "  if i < len(tokenized_datasetTexts)-1:\n",
        "    encoder_sentence = []\n",
        "    for j in range(NUM_VERSES_PREVIOUS_CONTEXT-1, -1, -1):\n",
        "      if i-j>=0:\n",
        "        if tokenized_datasetTexts[i-j] == tokenized_datasetTexts[0]:\n",
        "          encoder_sentence = tokenized_datasetTexts[i-j][1:-1] + [token_to_idx[LINE_SEPARATOR]]\n",
        "        else:\n",
        "          encoder_sentence += tokenized_datasetTexts[i-j][1:-1] + [token_to_idx[LINE_SEPARATOR]]      \n",
        "    encoder_input_texts.append(encoder_sentence)\n",
        "    max_encoder = max(max_encoder, len(encoder_input_texts[i]))\n",
        "  #decoder input and target\n",
        "  if i>0:\n",
        "    decoder_input_sentence = tokenized_datasetTexts[i][0:-1]\n",
        "    decoder_input_texts.append(decoder_input_sentence)\n",
        "    decoder_target_texts.append(tokenized_datasetTexts[i][1:])\n",
        "    max_decoder = max(max_decoder, len(decoder_input_texts[i-1]))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "JF2wgnaypWB2"
      },
      "source": [
        "### Definition of the dataset"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "huug2cMpoCEz"
      },
      "source": [
        "from tensorflow.keras.utils import Sequence\r\n",
        "import math\r\n",
        "import random\r\n",
        "\r\n",
        "class Seq2seqDataset(Sequence):\r\n",
        "    def __init__(self, encoder_input, decoder_input, decoder_target, max_encoder=None, max_decoder=None, batch_size=128, shuffle=False, tokenizer_for_rhyme=None):  \r\n",
        "      self.X = [encoder_input, decoder_input]\r\n",
        "      self.Y = decoder_target\r\n",
        "      self.batch_size = batch_size\r\n",
        "      self.indexes = list(range(len(self.Y)))\r\n",
        "      self.shuffle_after_epoch = shuffle\r\n",
        "      self.tokenizer_for_rhyme = tokenizer_for_rhyme\r\n",
        "\r\n",
        "    def shuffle(self):\r\n",
        "      random.shuffle(self.indexes)\r\n",
        "      return self\r\n",
        "\r\n",
        "    def on_epoch_end(self):\r\n",
        "      if self.shuffle_after_epoch:\r\n",
        "        self.shuffle()\r\n",
        "\r\n",
        "    def __len__(self):\r\n",
        "      return math.ceil(len(self.X[0])/self.batch_size)\r\n",
        "\r\n",
        "    def __getitem__(self, idx):\r\n",
        "      indexes = self.indexes[idx * self.batch_size:(idx + 1) * self.batch_size]\r\n",
        "      batch_encoder_input = pad_sequences([self.X[0][i] for i in indexes], maxlen=max_encoder, padding='pre', value=0, dtype=np.int32)\r\n",
        "      batch_decoder_input = pad_sequences([self.X[1][i] for i in indexes], maxlen=max_decoder, padding='pre', value=0, dtype=np.int32)\r\n",
        "      batch_decoder_target = pad_sequences([self.Y[i] for i in indexes], maxlen=max_decoder, padding='pre', value=0, dtype=np.int32)\r\n",
        "      if self.tokenizer_for_rhyme == None:\r\n",
        "        return [batch_encoder_input, batch_decoder_input], batch_decoder_target\r\n",
        "      else:\r\n",
        "        batch_encoder_rhyme_input = np.asarray([[self.tokenizer_for_rhyme[token] for token in text] for text in batch_encoder_input])\r\n",
        "        batch_decoder_rhyme_input = np.asarray([[self.tokenizer_for_rhyme[token] for token in text] for text in batch_decoder_input])\r\n",
        "        return [batch_encoder_input, batch_encoder_rhyme_input, batch_decoder_input, batch_decoder_rhyme_input], batch_decoder_target"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "95saticjzLXh",
        "outputId": "bc216cd4-b435-4a97-fe49-10d838eac8f3"
      },
      "source": [
        "encoder_input_data = np.asarray(encoder_input_texts)\r\n",
        "#encoder_input_rhyme_data = np.asarray(encoder_input_rhyme_texts)\r\n",
        "decoder_input_data = np.asarray(decoder_input_texts)\r\n",
        "#decoder_input_rhyme_data = np.asarray(decoder_input_rhyme_texts)\r\n",
        "decoder_target_data = np.asarray(decoder_target_texts)\r\n",
        "decoder_target_data = np.asarray([to_categorical(target, num_classes=vocab_size) for target in decoder_target_data])\r\n",
        "\r\n",
        "batch_size = 128\r\n",
        "\r\n",
        "validation = False \r\n",
        "valid_split = 0.1\r\n",
        "#split_treshold = int(encoder_input_data.shape[0]*(1-valid_split))\r\n",
        "#This treshold indicate where the first canto that will be present in validation set starts \r\n",
        "#(validation split is computed on the 100 canti instead of the total number of verses)\r\n",
        "split_treshold = 17007\r\n",
        "\r\n",
        "if validation:\r\n",
        "  if RHYME_ENCODING:\r\n",
        "    train_set = Seq2seqDataset(encoder_input_data[:split_treshold], \r\n",
        "                              decoder_input_data[:split_treshold], decoder_target_data[:split_treshold], \r\n",
        "                              max_encoder=max_encoder, max_decoder=max_decoder, batch_size=batch_size, tokenizer_for_rhyme=rhymeEncodingInputByWord)\r\n",
        "    valid_set = Seq2seqDataset(encoder_input_data[split_treshold:], decoder_input_data[split_treshold:], decoder_target_data[split_treshold:], \r\n",
        "                              max_encoder=max_encoder, max_decoder=max_decoder, batch_size=batch_size, tokenizer_for_rhyme=rhymeEncodingInputByWord)\r\n",
        "  else:\r\n",
        "    train_set = Seq2seqDataset(encoder_input_data[:split_treshold], decoder_input_data[:split_treshold], decoder_target_data[:split_treshold], \r\n",
        "                              max_encoder=max_encoder, max_decoder=max_decoder, batch_size=batch_size)\r\n",
        "    valid_set = Seq2seqDataset(encoder_input_data[split_treshold:], decoder_input_data[split_treshold:], decoder_target_data[split_treshold:], \r\n",
        "                              max_encoder=max_encoder, max_decoder=max_decoder, batch_size=batch_size)\r\n",
        "else:\r\n",
        "  if RHYME_ENCODING:\r\n",
        "    train_set = Seq2seqDataset(encoder_input_data, \r\n",
        "                              decoder_input_data, decoder_target_data, \r\n",
        "                              max_encoder=max_encoder, max_decoder=max_decoder, batch_size=batch_size, tokenizer_for_rhyme=rhymeEncodingInputByWord)\r\n",
        "  else:\r\n",
        "    train_set = Seq2seqDataset(encoder_input_data, decoder_input_data, decoder_target_data, max_encoder=max_encoder, \r\n",
        "                              max_decoder=max_decoder, batch_size=batch_size)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.6/dist-packages/numpy/core/_asarray.py:83: VisibleDeprecationWarning: Creating an ndarray from ragged nested sequences (which is a list-or-tuple of lists-or-tuples-or ndarrays with different lengths or shapes) is deprecated. If you meant to do this, you must specify 'dtype=object' when creating the ndarray\n",
            "  return array(a, dtype, copy=False, order=order)\n"
          ],
          "name": "stderr"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "daa3bratpKJ9"
      },
      "source": [
        "## Model definition"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "oz0ohh3spgGI"
      },
      "source": [
        "### Seq2Seq"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "UBLg4RLoIq0v",
        "outputId": "53540bfe-e702-4049-c2b1-1f6c4a8e2286"
      },
      "source": [
        "#param\n",
        "latent_dim = 128\n",
        "token_vector_space_dimension = 100\n",
        "\n",
        "# Define an input sequence and process it.\n",
        "encoder_inputs = Input(shape=(max_encoder), name=\"Encoder input\")\n",
        "embedding = Embedding(vocab_size, token_vector_space_dimension, mask_zero=True, name=\"Embedding\")\n",
        "encoder_embedding = embedding(encoder_inputs)\n",
        "\n",
        "#encoder = LSTM(latent_dim, dropout=0.2, return_sequences=True, return_state=True)\n",
        "#encoder_outputs, state_h, state_c = encoder(encoder_embedding)\n",
        "encoder = Bidirectional(LSTM(latent_dim, dropout=0.2, recurrent_dropout=0.2, return_sequences=True, return_state=True, name=\"Encoder\"))\n",
        "\n",
        "if RHYME_ENCODING:\n",
        "  encoder_inputs_rhyme = Input(shape=(max_encoder, word_max_len), name=\"Encoder input for rhyme\")\n",
        "  time_distributed_rhyme_model = TimeDistributed(rhyme_model)\n",
        "  encoder_input_rhyme = time_distributed_rhyme_model(encoder_inputs_rhyme)\n",
        "  encoder_inputs_with_rhyme_encoding = Concatenate()([encoder_embedding, encoder_input_rhyme]) \n",
        "  encoder_outputs, forward_h, forward_c, backward_h, backward_c = encoder(encoder_inputs_with_rhyme_encoding)\n",
        "else:\n",
        "  encoder_outputs, forward_h, forward_c, backward_h, backward_c = encoder(encoder_embedding)\n",
        "\n",
        "state_h = Concatenate()([forward_h, backward_h])\n",
        "state_c = Concatenate()([forward_c, backward_c])\n",
        "\n",
        "encoder_states = [state_h, state_c]\n",
        "\n",
        "# Set up the decoder, using `encoder_states` as initial state.\n",
        "decoder_inputs = Input(shape=(max_decoder), name=\"Decoder input\")\n",
        "# We set up our decoder to return full output sequences,\n",
        "# and to return internal states as well. We don't use the\n",
        "# return states in the training model, but we will use them in inference.\n",
        "decoder_embedding = embedding(decoder_inputs)\n",
        "decoder = LSTM(latent_dim*2, dropout=0.2, recurrent_dropout=0.2, return_sequences=True, return_state=True, name=\"Decoder\")\n",
        "\n",
        "if RHYME_ENCODING:\n",
        "  decoder_inputs_rhyme = Input(shape=(max_decoder, word_max_len), name=\"Decoder input for rhyme\")\n",
        "  decoder_input_rhyme = time_distributed_rhyme_model(decoder_inputs_rhyme)\n",
        "  decoder_inputs_with_rhyme_encoding = Concatenate()([decoder_embedding, decoder_input_rhyme])\n",
        "  decoder_outputs, decoder_state_h, _ = decoder(decoder_inputs_with_rhyme_encoding, initial_state=encoder_states)\n",
        "else:\n",
        "  decoder_outputs, decoder_state_h, _ = decoder(decoder_embedding, initial_state=encoder_states)\n",
        "\n",
        "# decoder1 = LSTM(latent_dim*2, return_sequences=True, return_state=True)\n",
        "# decoder2 = LSTM(latent_dim*2, return_sequences=True, return_state=True)\n",
        "# decoder_outputs, decoder_state_h, _ = decoder2(decoder1(decoder_embedding, initial_state=encoder_states))\n",
        "\n",
        "context_vector = Attention(dropout=0.2)([decoder_outputs, encoder_outputs])\n",
        "dense_input_layer = Concatenate()([decoder_outputs, context_vector])\n",
        "\n",
        "decoder_dense = Dense(vocab_size, activation='softmax')\n",
        "model_outputs = decoder_dense(dense_input_layer)\n",
        "\n",
        "# Define the model that will turn\n",
        "# `encoder_input_data` & `decoder_input_data` into `decoder_target_data`\n",
        "if RHYME_ENCODING:\n",
        "  model = Model([encoder_inputs, encoder_inputs_rhyme, decoder_inputs, decoder_inputs_rhyme], model_outputs)\n",
        "else:\n",
        "  model = Model([encoder_inputs, decoder_inputs], model_outputs)\n",
        "model.summary()"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "WARNING:tensorflow:Layer Encoder will not use cuDNN kernel since it doesn't meet the cuDNN kernel criteria. It will use generic GPU kernel as fallback when running on GPU\n",
            "WARNING:tensorflow:Layer Encoder will not use cuDNN kernel since it doesn't meet the cuDNN kernel criteria. It will use generic GPU kernel as fallback when running on GPU\n",
            "WARNING:tensorflow:Layer Encoder will not use cuDNN kernel since it doesn't meet the cuDNN kernel criteria. It will use generic GPU kernel as fallback when running on GPU\n",
            "WARNING:tensorflow:Layer Decoder will not use cuDNN kernel since it doesn't meet the cuDNN kernel criteria. It will use generic GPU kernel as fallback when running on GPU\n",
            "Model: \"model\"\n",
            "__________________________________________________________________________________________________\n",
            "Layer (type)                    Output Shape         Param #     Connected to                     \n",
            "==================================================================================================\n",
            "Decoder input (InputLayer)      [(None, 13)]         0                                            \n",
            "__________________________________________________________________________________________________\n",
            "Encoder input (InputLayer)      [(None, 33)]         0                                            \n",
            "__________________________________________________________________________________________________\n",
            "Embedding (Embedding)           multiple             1377900     Encoder input[0][0]              \n",
            "                                                                 Decoder input[0][0]              \n",
            "__________________________________________________________________________________________________\n",
            "bidirectional (Bidirectional)   [(None, 33, 256), (N 234496      Embedding[0][0]                  \n",
            "__________________________________________________________________________________________________\n",
            "concatenate (Concatenate)       (None, 256)          0           bidirectional[0][1]              \n",
            "                                                                 bidirectional[0][3]              \n",
            "__________________________________________________________________________________________________\n",
            "concatenate_1 (Concatenate)     (None, 256)          0           bidirectional[0][2]              \n",
            "                                                                 bidirectional[0][4]              \n",
            "__________________________________________________________________________________________________\n",
            "Decoder (LSTM)                  [(None, 13, 256), (N 365568      Embedding[1][0]                  \n",
            "                                                                 concatenate[0][0]                \n",
            "                                                                 concatenate_1[0][0]              \n",
            "__________________________________________________________________________________________________\n",
            "attention (Attention)           (None, 13, 256)      0           Decoder[0][0]                    \n",
            "                                                                 bidirectional[0][0]              \n",
            "__________________________________________________________________________________________________\n",
            "concatenate_2 (Concatenate)     (None, 13, 512)      0           Decoder[0][0]                    \n",
            "                                                                 attention[0][0]                  \n",
            "__________________________________________________________________________________________________\n",
            "dense (Dense)                   (None, 13, 13779)    7068627     concatenate_2[0][0]              \n",
            "==================================================================================================\n",
            "Total params: 9,046,591\n",
            "Trainable params: 9,046,591\n",
            "Non-trainable params: 0\n",
            "__________________________________________________________________________________________________\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "VcKMC8-apisw"
      },
      "source": [
        "### Training"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "p8z-JjgJ4MeP"
      },
      "source": [
        "model = loaded_model"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "BK7ZzOnA8FZm"
      },
      "source": [
        "epoch_interval = 10\n",
        "interval_batches_saving = len(train_set)*epoch_interval\n",
        "\n",
        "model_filepath=\"/content/gdrive/My Drive/Dante_model/WordSeq2SeqBiAttention_Dropout_0.2-final-{epoch:02d}+50.hdf5\"\n",
        "# Define Callbacks\n",
        "es = EarlyStopping(monitor='categorical_crossentropy', mode='min', verbose=2, patience=5)\n",
        "ck = ModelCheckpoint(model_filepath, verbose=1, save_freq=interval_batches_saving)\n",
        "# Run training\n",
        "model.compile(optimizer='adam', loss='categorical_crossentropy', metrics=['categorical_accuracy'])\n",
        "model.fit(train_set, epochs=70, callbacks=[ck, es, PlotLearning()])"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "8jAjtx5lYh87"
      },
      "source": [
        "from tensorflow.keras import backend as K\n",
        "def perplexity(y_true, y_pred):\n",
        "  return K.exp(K.mean(K.categorical_crossentropy(y_true, y_pred)))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "-XFNdur7pBYH"
      },
      "source": [
        "### Inference"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "IJ8yAEcdX0yl"
      },
      "source": [
        "attention = True\n",
        "latent_dim = 128\n",
        "\n",
        "def extract_inference_models(model, rhyme_encoding):\n",
        "  if rhyme_encoding:\n",
        "    #input of tokenized words\n",
        "    encoder_input = Input(shape=(max_encoder))\n",
        "    embedding = model.layers[4]\n",
        "    encoder_embedding = embedding(encoder_input)\n",
        "    #input for rhyme encoding\n",
        "    encoder_rhyme_input = Input(shape=(max_encoder, word_max_len))\n",
        "    encoder_rhyme_encoding = model.layers[5](encoder_rhyme_input)\n",
        "    encoder_refined_input = Concatenate()([encoder_embedding, encoder_rhyme_encoding])\n",
        "    #encoder\n",
        "    encoder_lstm = model.layers[7]\n",
        "    #encoder_outputs, state_h, state_c = encoder_lstm(encoder_embedding)\n",
        "    encoder_outputs, forward_h, forward_c, backward_h, backward_c = encoder_lstm(encoder_refined_input)\n",
        "    state_h = Concatenate()([forward_h, backward_h])\n",
        "    state_c = Concatenate()([forward_c, backward_c])\n",
        "    encoder_states = [state_h, state_c]\n",
        "    if attention:\n",
        "      encoder_model = Model([encoder_input, encoder_rhyme_input], [encoder_outputs, encoder_states])\n",
        "    else:\n",
        "      encoder_model = Model([encoder_input, encoder_rhyme_input], encoder_states)\n",
        "\n",
        "    decoder_state_input_h = Input(shape=(latent_dim*2))\n",
        "    decoder_state_input_c = Input(shape=(latent_dim*2))\n",
        "    decoder_states_inputs = [decoder_state_input_h, decoder_state_input_c]\n",
        "    decoder_input = Input(shape=(max_decoder))\n",
        "    decoder_embedding = embedding(decoder_input)\n",
        "\n",
        "    decoder_rhyme_input = Input(shape=(max_decoder, word_max_len))\n",
        "    decoder_rhyme_encoding = model.layers[5](decoder_rhyme_input)\n",
        "    decoder_refined_input = Concatenate()([decoder_embedding, decoder_rhyme_encoding])\n",
        "\n",
        "    decoder_lstm1 = model.layers[11]\n",
        "    dense_layer = model.layers[-1]\n",
        "    decoder_outputs, state_h_inf, state_c_inf = decoder_lstm1(decoder_refined_input, initial_state=decoder_states_inputs)\n",
        "\n",
        "    if attention:\n",
        "      encoder_state_input_for_attention = Input(shape=(max_encoder, latent_dim*2))\n",
        "      attention_inf = model.layers[12]\n",
        "      attention_output_inf = attention_inf([decoder_outputs, encoder_state_input_for_attention])\n",
        "      dense_input_layer_inf = Concatenate()([decoder_outputs, attention_output_inf])\n",
        "      decoder_states = [state_h_inf, state_c_inf]\n",
        "      model_outputs = dense_layer(dense_input_layer_inf)\n",
        "      decoder_model = Model([decoder_input, decoder_rhyme_input] + decoder_states_inputs + [encoder_state_input_for_attention], [model_outputs] + decoder_states)\n",
        "    else:\n",
        "      decoder_states = [state_h_inf, state_c_inf]\n",
        "      model_outputs = dense_layer(decoder_outputs)\n",
        "      decoder_model = Model([decoder_input, decoder_rhyme_input] + decoder_states_inputs, [model_outputs] + decoder_states)\n",
        "  #NOT Rhyme Encoding\n",
        "  else:\n",
        "    encoder_input = Input(shape=(max_encoder))\n",
        "    embedding = model.layers[2]\n",
        "    encoder_embedding = embedding(encoder_input)\n",
        "    encoder_lstm = model.layers[3]\n",
        "    #encoder_outputs, state_h, state_c = encoder_lstm(encoder_embedding)\n",
        "    encoder_outputs, forward_h, forward_c, backward_h, backward_c = encoder_lstm(encoder_embedding)\n",
        "    state_h = Concatenate()([forward_h, backward_h])\n",
        "    state_c = Concatenate()([forward_c, backward_c])\n",
        "    encoder_states = [state_h, state_c]\n",
        "    if attention:\n",
        "      encoder_model = Model(encoder_input, [encoder_outputs, encoder_states])\n",
        "    else:\n",
        "      encoder_model = Model(encoder_input, encoder_states)\n",
        "\n",
        "    decoder_state_input_h = Input(shape=(latent_dim*2))\n",
        "    decoder_state_input_c = Input(shape=(latent_dim*2))\n",
        "    decoder_states_inputs = [decoder_state_input_h, decoder_state_input_c]\n",
        "    decoder_input = Input(shape=(max_decoder))\n",
        "    decoder_embedding = embedding(decoder_input)\n",
        "    decoder_lstm1 = model.layers[6]\n",
        "    dense_layer = model.layers[-1]\n",
        "    decoder_outputs, state_h_inf, state_c_inf = decoder_lstm1(decoder_embedding, initial_state=decoder_states_inputs)\n",
        "\n",
        "    if attention:\n",
        "      encoder_state_input_for_attention = Input(shape=(max_encoder, latent_dim*2))\n",
        "      attention_inf = model.layers[7]\n",
        "      attention_output_inf = attention_inf([decoder_outputs, encoder_state_input_for_attention])\n",
        "      dense_input_layer_inf = Concatenate()([decoder_outputs, attention_output_inf])\n",
        "      decoder_states = [state_h_inf, state_c_inf]\n",
        "      model_outputs = dense_layer(dense_input_layer_inf)\n",
        "      decoder_model = Model([decoder_input] + decoder_states_inputs + [encoder_state_input_for_attention], [model_outputs] + decoder_states)\n",
        "    else:\n",
        "      decoder_states = [state_h_inf, state_c_inf]\n",
        "      model_outputs = dense_layer(decoder_outputs)\n",
        "      decoder_model = Model([decoder_input] + decoder_states_inputs, [model_outputs] + decoder_states)\n",
        "  return encoder_model, decoder_model"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "krmGJjNMPveH"
      },
      "source": [
        "encoder_model, decoder_model = extract_inference_models(model, False)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "pfxH3jZPEq41",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "6f5c7285-22b2-4e2a-b56d-1d85eab4a0ee"
      },
      "source": [
        "RHYME_ENCODING = False\n",
        "loaded_model = load_model('/content/gdrive/My Drive/Dante_model/WordSeq2SeqBiAttention-Dropout_0.3-final-110.hdf5')\n",
        "encoder_model, decoder_model = extract_inference_models(loaded_model, RHYME_ENCODING)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "WARNING:tensorflow:Layer Encoder will not use cuDNN kernel since it doesn't meet the cuDNN kernel criteria. It will use generic GPU kernel as fallback when running on GPU\n",
            "WARNING:tensorflow:Layer Encoder will not use cuDNN kernel since it doesn't meet the cuDNN kernel criteria. It will use generic GPU kernel as fallback when running on GPU\n",
            "WARNING:tensorflow:Layer Encoder will not use cuDNN kernel since it doesn't meet the cuDNN kernel criteria. It will use generic GPU kernel as fallback when running on GPU\n",
            "WARNING:tensorflow:Layer Decoder will not use cuDNN kernel since it doesn't meet the cuDNN kernel criteria. It will use generic GPU kernel as fallback when running on GPU\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Pg0u_6IhkNN9"
      },
      "source": [
        "#encoder_model.summary()\r\n",
        "#decoder_model.summary()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "gj-cUAlB8_S_",
        "outputId": "3a3ca0bc-3220-401c-adb4-49911587877c"
      },
      "source": [
        "def compute_rhyme_input_from_tokenized_input(tokenized_input):\n",
        "  return np.asarray([[rhymeEncodingInputByWord[token] for token in text] for text in tokenized_input])\n",
        "\n",
        "from tensorflow import make_ndarray, make_tensor_proto\n",
        "def decode_sequence(sequence, label = \"\"):\n",
        "  return tokenizer.sequences_to_texts([sequence])[0]\n",
        "\n",
        "def encode_sequence(text):\n",
        "  return tokenizer.texts_to_sequences([text])[0]\n",
        "  \n",
        "def generate_sequence(encoder_model, decoder_model, input_seq):\n",
        "    # Generate empty target sequence of length 1.\n",
        "    if  CHARACTER_BASED:\n",
        "      target_seq = tokenizer.texts_to_sequences(\"4\")\n",
        "    else:\n",
        "      target_seq = tokenizer.texts_to_sequences([\"<bol>\"])\n",
        "    \n",
        "    target_seq = pad_sequences(target_seq, maxlen=max_decoder, padding='pre', value=0)\n",
        "    input_seq = pad_sequences(input_seq, maxlen=max_encoder, padding='pre', value=0)\n",
        "\n",
        "    # Encode the input as state vectors.\n",
        "    if RHYME_ENCODING:\n",
        "      encoder_input = [input_seq, compute_rhyme_input_from_tokenized_input(input_seq)]\n",
        "    else:\n",
        "      encoder_input = [input_seq]\n",
        "    if attention:\n",
        "      encoder_outputs, states_value = encoder_model(encoder_input)\n",
        "    else:\n",
        "      states_value = encoder_model(input_seq)\n",
        "\n",
        "    attention_input = make_ndarray(make_tensor_proto(encoder_outputs))\n",
        "\n",
        "\n",
        "    #print(\"Input:\", input_seq.shape)\n",
        "    #print(\"Start: \", target_seq)\n",
        "\n",
        "    # Sampling loop for a batch of sequences\n",
        "    # (to simplify, here we assume a batch of size 1).\n",
        "    stop_condition = False\n",
        "    decoded_sentence = ''\n",
        "    i = 0\n",
        "    while not stop_condition:\n",
        "        if RHYME_ENCODING:\n",
        "          decoder_input = [target_seq, compute_rhyme_input_from_tokenized_input(target_seq)]\n",
        "        else:\n",
        "          decoder_input = [target_seq]\n",
        "        if attention:\n",
        "          output_tokens, h, c = decoder_model(decoder_input + states_value + [encoder_outputs])\n",
        "        else:\n",
        "          output_tokens, h, c = decoder_model(decoder_input + states_value)\n",
        "        # Sample a token\n",
        "        sampled_token_index = np.argmax(output_tokens[0, -1, :])\n",
        "        sampled_token = tokenizer.sequences_to_texts([[sampled_token_index]])[0]\n",
        "        #print(\"Output token: \" , sampled_token)\n",
        "\n",
        "        if (sampled_token == EOL or len(decoded_sentence) == max_decoder):\n",
        "            stop_condition = True\n",
        "        else:\n",
        "          decoded_sentence += sampled_token if CHARACTER_BASED else sampled_token + \" \"\n",
        "          i+=1\n",
        "          # Exit condition: either hit max length\n",
        "          # or find stop character.\n",
        "        \n",
        "          # Update the target sequence (of length 1).\n",
        "          #Pass only the last character generated to the decoder (suggested)\n",
        "          target_seq = tokenizer.texts_to_sequences([sampled_token])\n",
        "          #Pass all the character generated in the sequence to the decoder\n",
        "          #target_seq = [target_seq.tolist()[0] + tokenizer.texts_to_sequences(sampled_token)[0]]\n",
        "          target_seq = pad_sequences(target_seq, maxlen=max_decoder, truncating='pre', padding='pre', value=0)\n",
        "          # Update states\n",
        "          #print(h, \" + \", c, \" = \", h+context_vector)\n",
        "          states_value = [h, c]\n",
        "\n",
        "    #printSequenceFromEncodedArray(encoded_sentence, \"Encoded (copying during generation)\")\n",
        "    #print(encoded_sentence.shape)\n",
        "    return decoded_sentence\n",
        "\n",
        "def generate_sequence_beam_search(encoder_model, decoder_model, input_seq, k=2):\n",
        "    # Generate empty target sequence of length 1.\n",
        "    if  CHARACTER_BASED:\n",
        "      target_seq = tokenizer.texts_to_sequences(BOL)\n",
        "    else:\n",
        "      target_seq = tokenizer.texts_to_sequences([BOL])\n",
        "    \n",
        "    target_seq = pad_sequences(target_seq, maxlen=max_decoder, padding='pre', value=0)\n",
        "    input_seq = pad_sequences(input_seq, maxlen=max_encoder, padding='pre', value=0)\n",
        "\n",
        "    # Encode the input as state vectors.\n",
        "    if RHYME_ENCODING:\n",
        "      encoder_input = [input_seq, compute_rhyme_input_from_tokenized_input(input_seq)]\n",
        "    else:\n",
        "      encoder_input = [input_seq]\n",
        "    if attention:\n",
        "      encoder_outputs, states_value = encoder_model(encoder_input)\n",
        "    else:\n",
        "      states_value = encoder_model(input_seq)\n",
        "\n",
        "    #attention_input = make_ndarray(make_tensor_proto(encoder_outputs))\n",
        "    \n",
        "    decoded_sentence = ''\n",
        "    generated_output = dict()\n",
        "    queue = [{\"prob\": 0.0,\n",
        "          \"decoded_sentence\": \"\",\n",
        "          \"target_seq\": target_seq,\n",
        "          \"states\": states_value}]\n",
        "    i = 0\n",
        "    while len(queue) > 0:\n",
        "      next_step_queue = []\n",
        "      for context in queue:\n",
        "        if RHYME_ENCODING:\n",
        "          decoder_input = [context[\"target_seq\"], compute_rhyme_input_from_tokenized_input(target_seq)]\n",
        "        else:\n",
        "          decoder_input = [context[\"target_seq\"]]\n",
        "        if attention:\n",
        "          output_tokens, h, c = decoder_model(decoder_input + context[\"states\"] + [encoder_outputs])\n",
        "        else:\n",
        "          output_tokens, h, c = decoder_model(decoder_input + context[\"states\"])\n",
        "        # Sample a token\n",
        "        sampled_token_index = np.argmax(output_tokens[0, -1, :])\n",
        "        sampled_tokens_sorted = np.argsort(output_tokens[0, -1, :])[-k:]\n",
        "\n",
        "        decoded_sentence = context[\"decoded_sentence\"]\n",
        "        for sampled_token_index in sampled_tokens_sorted:\n",
        "          sampled_token = tokenizer.sequences_to_texts([[sampled_token_index]])[0]\n",
        "          if (sampled_token == EOL or i == max_decoder):\n",
        "            generated_output[context[\"prob\"]/(i+1)] = context[\"decoded_sentence\"]\n",
        "          else:\n",
        "            decoded_sentence += sampled_token if CHARACTER_BASED else sampled_token + \" \"\n",
        "          \n",
        "            # Update the target sequence (of length 1).\n",
        "            #Pass only the last character generated to the decoder (suggested)\n",
        "            target_seq = tokenizer.texts_to_sequences([sampled_token])\n",
        "            #Pass all the character generated in the sequence to the decoder\n",
        "            #target_seq = [target_seq.tolist()[0] + tokenizer.texts_to_sequences(sampled_token)[0]]\n",
        "            target_seq = pad_sequences(target_seq, maxlen=max_decoder, truncating='pre', padding='pre', value=0)\n",
        "            # Update states\n",
        "            #print(h, \" + \", c, \" = \", h+context_vector)\n",
        "            states_value = [h, c]\n",
        "\n",
        "            next_step_queue.append({\"prob\": np.log(output_tokens[0, -1, :][sampled_token_index])+context[\"prob\"],\n",
        "            \"decoded_sentence\": decoded_sentence,\n",
        "            \"target_seq\": target_seq,\n",
        "            \"states\": states_value})\n",
        "      if len(next_step_queue)==0:\n",
        "        queue = []\n",
        "      else:\n",
        "        queue = sorted(next_step_queue, key=lambda k: k['prob'])[:k]\n",
        "        next_step_queue = []\n",
        "      i+=1\n",
        "    next_sentence_max_prob = max([key for key in generated_output.keys()])\n",
        "    return generated_output[next_sentence_max_prob]\n",
        "\n",
        "generated_text = []\n",
        "generated_text.append(encoder_input_data[0])\n",
        "output = []\n",
        "print(\"start: \", tokenizer.sequences_to_texts([generated_text[0]])[0])\n",
        "decoded_sentence= \"\"\n",
        "while END not in decoded_sentence:\n",
        "  context = []\n",
        "  for i in range(0, len(generated_text)):\n",
        "    context += generated_text[i]\n",
        "  #print(\"Context passed: \", context)\n",
        "  decoded_sentence = generate_sequence_beam_search(encoder_model, decoder_model, [context], k=1)\n",
        "  if len(generated_text) == NUM_VERSES_PREVIOUS_CONTEXT:\n",
        "    generated_text.pop(0)\n",
        "  generated_text.append(encode_sequence(decoded_sentence + LINE_SEPARATOR))\n",
        "  print(decoded_sentence)\n",
        "  #print('Input sentence:', input_texts[seq_index])"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "start:  <start> $\n",
            "o poca nostra nobiltà di sangue \n",
            "che glorïar di te la gente fai \n",
            "qua giù dove l'affetto nostro langue \n",
            "<tercet> \n",
            "mirabil cosa non mi sarà mai \n",
            "ché là dove appetito non si torce \n",
            "dico nel cielo a me punir di gloriai \n",
            "<tercet> \n",
            "se tu se' che se' di là raccorce \n",
            "da la sua donna parte di fuor di costoro \n",
            "ma che la natura tua sì risponda \n",
            "<tercet> \n",
            "poco sofferse poi che non mi dice \n",
            "non basta da la bella persona \n",
            "prima che faccia li occhi suoi e 'l ventre \n",
            "<tercet> \n",
            "così vid' io te così vid' io corse \n",
            "vidi un di lor volume sia lasso \n",
            "sì che 'l convento de la mia radice \n",
            "<tercet> \n",
            "voi mi tacea e quel padre sterpi \n",
            "sovra noi dinanzi a noi un più partito \n",
            "e tue vidi già ch'al mio disire \n",
            "<tercet> \n",
            "io son vòlto i passi e li occhi al piè io \n",
            "per lo stremo del foco e quando i pesi \n",
            "e io e vidi un fei mi fei \n",
            "<tercet> \n",
            "mentre che la carità che fu quinci \n",
            "carcere vai per altezza d'ingegno \n",
            "mi disse tu se' che tu se' ben certo \n",
            "<tercet> \n",
            "e io a lui tuo tuo veder li occhi \n",
            "che vïolenza in giuso a la morte folle \n",
            "ch'alcuna ch'ad esso esser son carca \n",
            "<tercet> \n",
            "sì come 'n su la costa scogli èe \n",
            "che 'l muover non torse ch'a sé fece \n",
            "poi che 'n vista cercar si tace \n",
            "<tercet> \n",
            "noi siam l'amor che sì li vostri \n",
            "per quel ch'io veggio e dicea da noi sospinto \n",
            "perch' ha del monte e de li uomini dèi \n",
            "<tercet> \n",
            "e pare al mio duca e io scoppio \n",
            "che fosti in terra e dentro ad esso ad una \n",
            "apparecchiava grazïoso loco \n",
            "<tercet> \n",
            "el cominciò figliuol mio frate \n",
            "quelli m'abellis vostre cortes deman \n",
            "qu'ieu no me puesc ni voill a vos cobrire \n",
            "<tercet> \n",
            "ieu sui arnaut que plor e vau cantan \n",
            "consiros vei la passada folor \n",
            "e vei jausen lo joi qu'esper denan \n",
            "<tercet> \n",
            "ara vos prec per aquella valor \n",
            "que vos guida al som de l'escalina \n",
            "sovenha vos a temps de ma dolor \n",
            "<tercet> \n",
            "poi s'ascose nel foco che li affina \n",
            "<end> \n"
          ],
          "name": "stdout"
        }
      ]
    }
  ]
}